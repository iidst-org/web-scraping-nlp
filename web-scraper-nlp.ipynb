{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5s-UF1Lmw474"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "def extract_header_and_div_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises an HTTPError if the response status code is 4XX/5XX\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extracting text from the <header> tag\n",
        "        header = soup.find('h1')\n",
        "        header_text = header.get_text(strip=True) if header else \"No header found\"\n",
        "\n",
        "        # Extracting text from a <div> with a specific class\n",
        "        div = soup.find('div', class_='td-post-content tagdiv-type')  # Replace 'article-text-class' with the actual class name\n",
        "        div_text = div.get_text(strip=True) if div else \"No text found in specified div\"\n",
        "\n",
        "        return header_text, div_text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request error: {e}\")\n",
        "        return \"\", \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "        return \"\", \"\"\n",
        "\n",
        "csv_file_path = 'Input Sheet1.csv'\n",
        "output_folder_path = 'ExtractedTexts'  # Folder where individual files will be saved\n",
        "\n",
        "# Ensure the output folder exists\n",
        "os.makedirs(output_folder_path, exist_ok=True)\n",
        "\n",
        "with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "        article_id, url = row\n",
        "        header_text, div_text = extract_header_and_div_text_from_url(url)\n",
        "\n",
        "        # Creating a unique filename for each article\n",
        "        filename = f\"{output_folder_path}/{article_id}.txt\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as outputfile:\n",
        "            outputfile.write(f\"URL: {url}\\n\")\n",
        "            outputfile.write(f\"Header: {header_text}\\n\")\n",
        "            outputfile.write(f\"Div Text: {div_text}\\n\")\n",
        "\n",
        "        print(f\"Saved: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sU8QUbZTctTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "shutil.make_archive(\"cleaned files\", 'zip', \"cleaned files\")\n",
        "files.download(\"cleaned files\")"
      ],
      "metadata": {
        "id": "X29qOOQPcj3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36 and 48 not found"
      ],
      "metadata": {
        "id": "AxyaQiYKeok6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Paths to the directories\n",
        "source_folder = '/content/ExtractedTexts'\n",
        "stopwords_folder = '/content/stopwords'\n",
        "cleaned_folder = '/content/cleaned files'\n",
        "\n",
        "# Ensure the cleaned files folder exists\n",
        "if not os.path.exists(cleaned_folder):\n",
        "    os.makedirs(cleaned_folder)\n",
        "\n",
        "# Load stopwords\n",
        "stopwords = set()\n",
        "for filename in os.listdir(stopwords_folder):\n",
        "    # Check if it's a file, not a directory\n",
        "    if os.path.isfile(os.path.join(stopwords_folder, filename)):\n",
        "        # Try to detect the encoding, fallback to 'latin-1' if detection fails\n",
        "        try:\n",
        "            with open(os.path.join(stopwords_folder, filename), 'r', encoding='utf-8') as file:\n",
        "                for line in file:\n",
        "                    stopwords.add(line.strip().lower())\n",
        "        except UnicodeDecodeError:\n",
        "            with open(os.path.join(stopwords_folder, filename), 'r', encoding='latin-1') as file:\n",
        "                for line in file:\n",
        "                    stopwords.add(line.strip().lower())\n",
        "\n",
        "# Function to clean a single file\n",
        "def clean_file(file_path, stopwords):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "    words = content.split()\n",
        "    cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "# Clean each file in the source folder and save in the cleaned folder\n",
        "for filename in os.listdir(source_folder):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(source_folder, filename)\n",
        "        cleaned_content = clean_file(file_path, stopwords)\n",
        "        # Construct new file path in the cleaned folder\n",
        "        new_file_path = os.path.join(cleaned_folder, filename)\n",
        "        # Save the cleaned content to the new file\n",
        "        with open(new_file_path, 'w') as file:\n",
        "            file.write(cleaned_content)"
      ],
      "metadata": {
        "id": "6Z92sgTVhKHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "# Paths to the directories\n",
        "cleaned_folder = '/content/cleaned files'\n",
        "positive_words_file = '/content/positive-words.txt'\n",
        "negative_words_file = '/content/negative-words.txt'\n",
        "results_file = '/content/sentiment-results.csv'\n",
        "\n",
        "# Load positive and negative words\n",
        "positive_words = set()\n",
        "negative_words = set()\n",
        "\n",
        "with open(positive_words_file, 'r') as file:\n",
        "    for line in file:\n",
        "        positive_words.add(line.strip().lower())\n",
        "\n",
        "# Read negative words, trying UTF-8 first, then falling back to latin-1\n",
        "try:\n",
        "    with open(negative_words_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            negative_words.add(line.strip().lower())\n",
        "except UnicodeDecodeError:\n",
        "    with open(negative_words_file, 'r', encoding='latin-1') as file:\n",
        "        for line in file:\n",
        "            negative_words.add(line.strip().lower())\n",
        "\n",
        "# Function to analyze sentiment of a single file\n",
        "def analyze_sentiment(file_path, positive_words, negative_words):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "    words = content.split()\n",
        "    positive_score = sum(1 for word in words if word.lower() in positive_words)\n",
        "    negative_score = sum(1 for word in words if word.lower() in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    # Calculate Subjectivity Score\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
        "\n",
        "# Open the results file and write the headers\n",
        "with open(results_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['File Name', 'Positive Score', 'Negative Score', 'Polarity Score', 'Subjectivity Score'])\n",
        "\n",
        "    # Analyze sentiment for each cleaned file and write the results\n",
        "    for filename in os.listdir(cleaned_folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(cleaned_folder, filename)\n",
        "            positive_score, negative_score, polarity_score, subjectivity_score = analyze_sentiment(file_path, positive_words, negative_words)\n",
        "            writer.writerow([filename, positive_score, negative_score, polarity_score, subjectivity_score])\n",
        "\n",
        "print(f\"Sentiment analysis results saved to {results_file}\")"
      ],
      "metadata": {
        "id": "dYz07QjfyMqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install syllables"
      ],
      "metadata": {
        "id": "ezQiAGEu3wVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import syllables\n",
        "import re\n",
        "\n",
        "# Paths to the directories\n",
        "cleaned_folder = '/content/cleaned files'\n",
        "results_file = '/content/readability-results.csv'\n",
        "\n",
        "# Function to calculate readability metrics for a single file\n",
        "def analyze_readability(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    words = content.split()\n",
        "    sentences = content.split('.')  # Simple sentence splitting, might need refinement\n",
        "    num_words = len(words)\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    # Calculate Average Sentence Length\n",
        "    average_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "    # Calculate Percentage of Complex Words and count complex words\n",
        "    num_complex_words = 0\n",
        "    total_syllables = 0\n",
        "    total_characters = 0\n",
        "    for word in words:\n",
        "      syllable_count = syllables.estimate(word)\n",
        "      total_syllables += syllable_count\n",
        "      total_characters += len(word)\n",
        "      if syllable_count > 2:\n",
        "        num_complex_words += 1\n",
        "    percentage_complex_words = (num_complex_words / num_words) * 100 if num_words > 0 else 0\n",
        "\n",
        "    # Calculate Fog Index\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "    # Calculate Average Word Length\n",
        "    average_word_length = total_characters / num_words if num_words > 0 else 0\n",
        "\n",
        "    # Calculate Personal Pronouns using regex\n",
        "    personal_pronoun_pattern = r\"\\b(I|me|my|mine|myself|we|us|our|ours|ourselves|you|your|yours|yourself|yourselves|he|him|his|himself|she|her|hers|herself|it|its|itself|they|them|their|theirs|themselves)\\b\"\n",
        "    personal_pronouns = re.findall(personal_pronoun_pattern, content, flags=re.IGNORECASE)\n",
        "    num_personal_pronouns = len(personal_pronouns)\n",
        "\n",
        "    return average_sentence_length, percentage_complex_words, fog_index, num_complex_words, num_words, total_syllables, average_word_length, num_personal_pronouns\n",
        "\n",
        "# Open the results file and write the headers\n",
        "with open(results_file, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['File Name', 'Average Sentence Length', 'Percentage of Complex Words', 'Fog Index', 'Complex Word Count', 'Word Count', 'Syllable Count', 'Average Word Length', 'Personal Pronouns'])\n",
        "\n",
        "    # Analyze readability for each cleaned file and write the results\n",
        "    for filename in os.listdir(cleaned_folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(cleaned_folder, filename)\n",
        "            avg_sentence_length, pct_complex_words, fog_index, num_complex_words, num_words, total_syllables, avg_word_length, num_personal_pronouns = analyze_readability(file_path)\n",
        "            writer.writerow([filename, avg_sentence_length, pct_complex_words, fog_index, num_complex_words, num_words, total_syllables, avg_word_length, num_personal_pronouns])\n",
        "            print(f\"For file {filename}: Complex words: {num_complex_words}, Word count: {num_words}, Syllable count: {total_syllables}, Average word length: {avg_word_length:.2f}, Personal pronouns: {num_personal_pronouns}\")\n",
        "\n",
        "print(f\"Readability analysis results saved to {results_file}\")"
      ],
      "metadata": {
        "id": "k6TZqZ393hwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing all required libraries"
      ],
      "metadata": {
        "id": "pKAWm-60xfow"
      }
    }
  ]
}